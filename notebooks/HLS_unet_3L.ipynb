{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307547bb-19fa-4a5b-835c-49df2bead4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "\n",
    "from qonnx.transformation.fold_constants import FoldConstants\n",
    "\n",
    "from qonnx.transformation.general import (\n",
    "    ConvertSubToAdd,\n",
    "    ConvertDivToMul,\n",
    "    GiveReadableTensorNames,\n",
    "    GiveUniqueNodeNames,\n",
    "    SortGraph,\n",
    "    RemoveUnusedTensors,\n",
    "    GiveUniqueParameterTensors,\n",
    "    RemoveStaticGraphInputs,\n",
    "    ApplyConfig,\n",
    ")\n",
    "\n",
    "from finn.transformation.streamline.absorb import (\n",
    "    AbsorbScalarMulAddIntoTopK,\n",
    "    AbsorbAddIntoMultiThreshold,\n",
    "    AbsorbMulIntoMultiThreshold,\n",
    "    FactorOutMulSignMagnitude,\n",
    "    Absorb1BitMulIntoMatMul,\n",
    "    Absorb1BitMulIntoConv,\n",
    "    AbsorbConsecutiveTransposes,\n",
    "    AbsorbTransposeIntoMultiThreshold,\n",
    "    AbsorbSignBiasIntoMultiThreshold\n",
    ")\n",
    "\n",
    "from finn.transformation.streamline.collapse_repeated import (\n",
    "    CollapseRepeatedAdd,\n",
    "    CollapseRepeatedMul,\n",
    ")\n",
    "\n",
    "from finn.transformation.streamline.reorder import (\n",
    "    MoveAddPastMul,\n",
    "    MoveScalarMulPastMatMul,\n",
    "    MoveScalarAddPastMatMul,\n",
    "    MoveAddPastConv,\n",
    "    MoveScalarMulPastConv,\n",
    "    MoveScalarLinearPastInvariants,\n",
    "    MoveMaxPoolPastMultiThreshold,\n",
    "    MakeScaleResizeNHWC,\n",
    "    MoveMulPastMaxPool,\n",
    "    MakeMaxPoolNHWC,\n",
    ")\n",
    "\n",
    "from finn.transformation.streamline.round_thresholds import RoundAndClipThresholds\n",
    "from finn.transformation.streamline.sign_to_thres import ConvertSignToThres\n",
    "from qonnx.transformation.batchnorm_to_affine import BatchNormToAffine\n",
    "\n",
    "from qonnx.transformation.make_input_chanlast import MakeInputChannelsLast\n",
    "\n",
    "# just for not linear\n",
    "from finn.transformation.streamline.reorder import (\n",
    "    MoveLinearPastEltwiseAdd,\n",
    "    MoveLinearPastFork,\n",
    ")\n",
    "\n",
    "from qonnx.transformation.double_to_single_float import DoubleToSingleFloat\n",
    "from qonnx.transformation.remove import RemoveIdentityOps\n",
    "from qonnx.core.datatype import DataType\n",
    "\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "from qonnx.transformation.infer_data_layouts import InferDataLayouts\n",
    "from qonnx.transformation.insert_topk import InsertTopK\n",
    "import finn.transformation.fpgadataflow.convert_to_hls_layers as to_hls\n",
    "from qonnx.transformation.lower_convs_to_matmul import LowerConvsToMatMul\n",
    "\n",
    "from finn.builder.build_dataflow_config import (\n",
    "    DataflowBuildConfig,\n",
    "    ShellFlowType,\n",
    ")\n",
    "\n",
    "from finn.transformation.fpgadataflow.prepare_ip import PrepareIP\n",
    "from finn.transformation.fpgadataflow.hlssynth_ip import HLSSynthIP\n",
    "from finn.transformation.fpgadataflow.replace_verilog_relpaths import (\n",
    "    ReplaceVerilogRelPaths,\n",
    ")\n",
    "\n",
    "from finn.transformation.move_reshape import RemoveCNVtoFCFlatten\n",
    "\n",
    "from qonnx.util.config import extract_model_config_to_json\n",
    "from finn.transformation.fpgadataflow.set_fifo_depths import (\n",
    "    InsertAndSetFIFODepths,\n",
    "    RemoveShallowFIFOs,\n",
    "    SplitLargeFIFOs,\n",
    ")\n",
    "from finn.transformation.fpgadataflow.insert_dwc import InsertDWC\n",
    "from finn.transformation.fpgadataflow.insert_fifo import InsertFIFO\n",
    "from finn.transformation.fpgadataflow.create_dataflow_partition import CreateDataflowPartition\n",
    "from finn.util.visualization import showSrc, showInNetron\n",
    "from finn.custom_op.fpgadataflow.matrixvectoractivation import MatrixVectorActivation as MVAU\n",
    "from finn.custom_op.fpgadataflow.thresholding_batch import Thresholding_Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a949f323-df21-4a17-8d25-836c5e5fd43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import onnx\n",
    "import torch\n",
    "import brevitas\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Target device: \" + str(device))\n",
    "print(torch.__version__)\n",
    "print(brevitas.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb981257-89e2-4248-a4ad-6fc437652c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import brevitas.nn as qnn\n",
    "from brevitas.quant import Int8Bias\n",
    "from brevitas.core.quant import QuantType\n",
    "from brevitas.quant import (Int8ActPerTensorFloat, \n",
    "                            SignedBinaryWeightPerTensorConst, \n",
    "                            SignedTernaryWeightPerTensorConst,\n",
    "                            SignedBinaryActPerTensorConst)\n",
    "\n",
    "set_weight_bit_width = 4\n",
    "set_activation_bit_width = 4\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.double_conv = nn.Sequential(\n",
    "            \n",
    "            qnn.QuantConv2d(in_channels, out_channels, kernel_size = 3, weight_bit_width=set_weight_bit_width,\n",
    "                            padding = 1, bias=False, return_quant_tensor=True),\n",
    "            #qnn.QuantConv2d(in_channels, out_channels, kernel_size = 3, weight_quant=SignedBinaryWeightPerTensorConst,\n",
    "            #                padding = 1, bias=False, return_quant_tensor=True),\n",
    "            #nn.BatchNorm2d(out_channels),\n",
    "            qnn.QuantReLU(bit_width=set_activation_bit_width, return_quant_tensor=True),\n",
    "            \n",
    "            qnn.QuantConv2d(out_channels, out_channels, kernel_size = 3, weight_bit_width=set_weight_bit_width,\n",
    "                            padding = 1, bias=False, return_quant_tensor=True),\n",
    "            #qnn.QuantConv2d(out_channels, out_channels, kernel_size = 3, weight_quant=SignedBinaryWeightPerTensorConst,\n",
    "            #                padding = 1, bias=False, return_quant_tensor=True),\n",
    "            #nn.BatchNorm2d(out_channels),\n",
    "            qnn.QuantReLU(bit_width=set_activation_bit_width, return_quant_tensor=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Encoder_1(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            \n",
    "            nn.MaxPool2d(2), \n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Encoder_2(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            \n",
    "            nn.MaxPool2d(3), \n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Decoder_1(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.up = nn.Sequential(\n",
    "            \n",
    "            qnn.QuantConv2d(in_channels, in_channels//2, kernel_size=1, weight_bit_width=set_weight_bit_width, \n",
    "                            bias=False, return_quant_tensor=True),\n",
    "            #qnn.QuantConv2d(in_channels, in_channels//2, kernel_size=1, weight_quant=SignedBinaryWeightPerTensorConst, \n",
    "            #                bias=False, return_quant_tensor=True),\n",
    "            nn.BatchNorm2d(in_channels//2),\n",
    "            qnn.QuantReLU(bit_width=set_activation_bit_width, return_quant_tensor=True),\n",
    "            qnn.QuantUpsamplingNearest2d(scale_factor=3, return_quant_tensor=True)\n",
    "        )\n",
    "        self.conv = DoubleConv(in_channels//2, out_channels)\n",
    "        self.quant_inp = qnn.QuantIdentity(act_quant = Int8ActPerTensorFloat, bit_width=set_activation_bit_width, return_quant_tensor=True)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # x1 is the feature map after upsampling\n",
    "        # x2 is the feature map from the corresponding layer in the downsampling path for concatenation\n",
    "        x1 = self.up(x1)\n",
    "        x1 = self.quant_inp(x1)\n",
    "        x2 = self.quant_inp(x2)\n",
    "        x = x1 + x2\n",
    "        x = self.quant_inp(x)\n",
    "        \n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Decoder_2(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.up = nn.Sequential(\n",
    "            \n",
    "            qnn.QuantConv2d(in_channels, in_channels, kernel_size=1, weight_bit_width=set_weight_bit_width, \n",
    "                            bias=False,  return_quant_tensor=True),\n",
    "            #qnn.QuantConv2d(in_channels, in_channels, kernel_size=1, weight_quant=SignedBinaryWeightPerTensorConst, \n",
    "            #                bias=False,  return_quant_tensor=True),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            qnn.QuantReLU(bit_width=set_activation_bit_width, return_quant_tensor=True),\n",
    "            qnn.QuantUpsamplingNearest2d(scale_factor=2, return_quant_tensor=True)\n",
    "        )\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "        self.quant_inp = qnn.QuantIdentity(act_quant = Int8ActPerTensorFloat, bit_width=set_activation_bit_width, return_quant_tensor=True)\n",
    " \n",
    "    def forward(self, x1, x2):\n",
    "        # x1 is the feature map after upsampling\n",
    "        # x2 is the feature map from the corresponding layer in the downsampling path for concatenation\n",
    "        x1 = self.up(x1)\n",
    "        x1 = self.quant_inp(x1)\n",
    "        x2 = self.quant_inp(x2)\n",
    "        x = x1 + x2\n",
    "        x = self.quant_inp(x)\n",
    "        \n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = qnn.QuantConv2d(in_channels, out_channels, kernel_size = 1, weight_bit_width=set_weight_bit_width, bias=False)\n",
    "        #self.conv = qnn.QuantConv2d(in_channels, out_channels, kernel_size = 1, weight_quant=SignedBinaryWeightPerTensorConst, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6f2bdf-4b1b-4a71-9054-401279a85774",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantWeightActUNet_one4all(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "\n",
    "        self.quant_inp = qnn.QuantIdentity(bit_width=4, return_quant_tensor=True)\n",
    "        self.input_double_conv = DoubleConv(input_channels, 16)\n",
    "        self.encoder_block1 = Encoder_1(16, 32)\n",
    "        self.encoder_block2 = Encoder_2(32, 64)\n",
    "        self.decoder_block3 = Decoder_1(64, 16)\n",
    "        self.decoder_block4 = Decoder_2(16, 8)\n",
    "        self.output_conv = OutConv(8, output_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant_inp(x)\n",
    "        x1 = self.input_double_conv(x)\n",
    "        x2 = self.encoder_block1(x1)\n",
    "        x3 = self.encoder_block2(x2)\n",
    "        x = self.decoder_block3(x3, x2)\n",
    "        x = self.decoder_block4(x, x1)\n",
    "        out = self.output_conv(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602ef652-870b-4473-a870-433e6861f50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QuantWeightActUNet_one4all(2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d02d84-ec40-4f52-a620-05f137eed920",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "data_list = []\n",
    "ancilla_list = []\n",
    "\n",
    "num_samples = 1000\n",
    "\n",
    "save_dir = \"../rotSC_sample4D_01p/\"\n",
    "\n",
    "for i in range(num_samples):\n",
    "    filename_data = os.path.join(save_dir, f'data_sample4D_{i}.npy')\n",
    "    data = np.load(filename_data)\n",
    "    filename_ancilla = os.path.join(save_dir, f'ancilla_sample4D_{i}.npy')\n",
    "    ancilla = np.load(filename_ancilla)\n",
    "    data_list.append(data.squeeze(1))\n",
    "    ancilla_list.append(ancilla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15fc56f-92fb-4b40-bbc9-12f89994c2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_list = []\n",
    "test_ancilla_list = []\n",
    "\n",
    "num_samples_test = 100\n",
    "\n",
    "save_dir_test = \"../rotSC_sample4D_test_01p/\"\n",
    "\n",
    "for i in range(num_samples_test):\n",
    "    filename_data = os.path.join(save_dir_test, f'data_sample4D_test_{i}.npy')\n",
    "    data = np.load(filename_data)\n",
    "    filename_ancilla = os.path.join(save_dir_test, f'ancilla_sample4D_test_{i}.npy')\n",
    "    ancilla = np.load(filename_ancilla)\n",
    "    test_data_list.append(data.squeeze(1))\n",
    "    test_ancilla_list.append(ancilla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3b303f-e841-455c-895d-1ef2e224b444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ancilla as input, data as target\n",
    "data_np = np.concatenate(data_list, axis=0)\n",
    "data_tensor = torch.tensor(data_np, dtype=torch.long)\n",
    "padded_data_tensor = F.pad(data_tensor, (0, 1, 0, 1), \"constant\", 0)\n",
    "\n",
    "ancilla_np = np.concatenate(ancilla_list, axis=0)\n",
    "ancilla_tensor = torch.tensor(ancilla_np, dtype=torch.float32)\n",
    "\n",
    "train_quantized_dataset = TensorDataset(ancilla_tensor, padded_data_tensor)\n",
    "\n",
    "test_data_np = np.concatenate(test_data_list, axis=0)\n",
    "test_data_tensor = torch.tensor(test_data_np, dtype=torch.long)\n",
    "padded_test_data_tensor = F.pad(test_data_tensor, (0, 1, 0, 1), \"constant\", 0)\n",
    "\n",
    "test_ancilla_np = np.concatenate(test_ancilla_list, axis=0)\n",
    "test_ancilla_tensor = torch.tensor(test_ancilla_np, dtype=torch.float32)\n",
    "\n",
    "test_quantized_dataset = TensorDataset(test_ancilla_tensor, padded_test_data_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174bbd3c-2b1a-499a-ad97-66cc555e3fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# dataset loaders\n",
    "train_quantized_loader = DataLoader(train_quantized_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_quantized_loader = DataLoader(test_quantized_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015c850c-d2ce-4cb6-a11a-4dcc69e08e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion):\n",
    "    losses = []\n",
    "    # ensure model is in training mode\n",
    "    model.train()    \n",
    "    \n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        input, target = input.to(device), target.to(device)\n",
    "        optimizer.zero_grad()   \n",
    "                \n",
    "        # forward pass\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # backward pass + run optimizer to update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # keep track of loss value\n",
    "        losses.append(loss.data.cpu().numpy()) \n",
    "           \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528fd076-0370-455d-a954-dd47c861021d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, test_loader, criterion):\n",
    "    losses = []\n",
    "    # ensure model is in training mode\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        for i, (input, target) in enumerate(test_loader):        \n",
    "            input , target = input.to(device), target.to(device) \n",
    "                    \n",
    "            # forward pass\n",
    "            #ee_choice = 'bb'\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)    \n",
    "            # keep track of loss value\n",
    "            losses.append(loss.data.cpu().numpy()) \n",
    "            # test accuracy\n",
    "            _, predicted = output.max(dim=1)\n",
    "            check_each_h = (predicted == target).all(dim=1)\n",
    "            check_each_w = check_each_h.all(dim=1)\n",
    "            correct += check_each_w.sum().item()\n",
    "            total += target.size(0)\n",
    "            \n",
    "    accuracy = correct / total\n",
    "           \n",
    "    return losses, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febb027f-00b2-4444-82e3-b102b790b0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "U_net_model = QuantWeightActUNet_one4all(2, 4)\n",
    "U_net_model.to(device)\n",
    "num_epochs = 50\n",
    "running_loss_train = []\n",
    "running_loss_validate = []\n",
    "running_test_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978407f1-e1bf-4a62-85e6-c511ef874f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# learning rate: the size of the steps that the optimizer takes along the gradient towards minimizing the loss function.\n",
    "lr = 0.01\n",
    "# loss criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(U_net_model.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "scheduler = StepLR(optimizer, step_size=25, gamma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e4f70e-afb7-4155-b093-5f9d792e837b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ranhuo/.local/lib/python3.8/site-packages/torch/_tensor.py:1394: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at ../c10/core/TensorImpl.h:1908.)\n",
      "  return super().rename(names)\n",
      "Training loss = 0.002266, Validation loss = 0.030074, accuracy = 0.920000: 100%|██████████| 50/50 [00:31<00:00,  1.57it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "# Setting seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "t = trange(num_epochs, desc=\"Training loss\", leave=True)\n",
    "\n",
    "for epoch in t:\n",
    "    loss_epoch_train = train(U_net_model, train_quantized_loader, optimizer, criterion)\n",
    "    loss_epoch_validation, accuracy = validate(U_net_model, test_quantized_loader, criterion)\n",
    "    scheduler.step()\n",
    "    #accuracy = test(U_net_model, test_quantized_loader)\n",
    "    t.set_description(\"Training loss = %f, Validation loss = %f, accuracy = %f\" % (np.mean(loss_epoch_train), np.mean(loss_epoch_validation), accuracy))\n",
    "    #t.set_description(\"Training loss = %f\" % (np.mean(loss_epoch_train)))\n",
    "    t.refresh() # to show immediately the update\n",
    "    running_loss_train.append(loss_epoch_train)\n",
    "    running_loss_validate.append(loss_epoch_validation)\n",
    "    running_test_acc.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51615b6c-d2ac-4b1a-8927-e65ad957a08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_loss_plot(losses_1, losses_2, title, xlabel=\"Iterations\", ylabel=\"Loss\"):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    x_axis = [i for i in range(len(losses_1))]\n",
    "    plt.plot(x_axis, losses_1, label = \"Training loss\")\n",
    "    plt.plot(x_axis, losses_2, label = \"Validation loss\")\n",
    "    plt.title(title)\n",
    "    plt.ylim(0, 0.03)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "train_loss_per_epoch = [np.mean(train_loss_per_epoch) for train_loss_per_epoch in running_loss_train]\n",
    "validate_loss_per_epoch = [np.mean(validate_loss_per_epoch) for validate_loss_per_epoch in running_loss_validate]\n",
    "display_loss_plot(train_loss_per_epoch, validate_loss_per_epoch, \"Training and Validation Loss Over Epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb395481-c6d6-4c35-83f9-c940bc552e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_accu_plot(accu, losses_1, losses_2, title, xlabel=\"Iterations\", ylabel1=\"Accuracy\", ylabel2=\"Loss\"):\n",
    "    fig, ax1 = plt.subplots(figsize=(8, 5))\n",
    "    x_axis = [i for i in range(len(accu))]\n",
    "    ax1.plot(x_axis, accu, color = \"green\", label = \"Accuracy\")\n",
    "    ax1.set_xlabel(xlabel)\n",
    "    ax1.set_ylabel(ylabel1, color='green')\n",
    "    ax1.tick_params(axis='y', labelcolor='green')\n",
    "    \n",
    "    # Find and mark the maximum accuracy\n",
    "    max_accu = max(accu)\n",
    "    max_index = accu.index(max_accu)\n",
    "    ax1.scatter(max_index, max_accu, color='green')\n",
    "    ax1.text(max_index, max_accu, f'{max_accu:.2f}', ha='left', color='green')\n",
    "    \n",
    "    # Create a second y-axis for the loss\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(x_axis, losses_1, color='red', label = \"Training loss\")\n",
    "    ax2.plot(x_axis, losses_2, color='orange', label = \"Validation loss\")\n",
    "    ax2.set_ylabel(ylabel2, color='red')\n",
    "    ax2.tick_params(axis='y', labelcolor='red')\n",
    "    ax2.set_ylim(0, 0.03)\n",
    "\n",
    "    ax1.legend(loc='lower left')\n",
    "    ax2.legend(loc='lower right')\n",
    "    \n",
    "    plt.title(title)\n",
    "    fig.tight_layout()  # Adjust layout to make room for both y-labels\n",
    "    plt.show()\n",
    "    \n",
    "accu_per_epoch = [np.mean(accu_per_epoch) for accu_per_epoch in running_test_acc]\n",
    "train_loss_per_epoch = [np.mean(train_loss_per_epoch) for train_loss_per_epoch in running_loss_train]\n",
    "validate_loss_per_epoch = [np.mean(validate_loss_per_epoch) for validate_loss_per_epoch in running_loss_validate]\n",
    "display_accu_plot(accu_per_epoch, train_loss_per_epoch, validate_loss_per_epoch, \"Accuracy and Loss Over Epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753872ef-4144-4796-8b22-b1d8aebb2c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.util.test import get_test_model_trained\n",
    "from brevitas.export import export_qonnx\n",
    "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.transformation.fold_constants import FoldConstants\n",
    "from qonnx.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames, RemoveStaticGraphInputs\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "from qonnx.transformation.infer_data_layouts import InferDataLayouts\n",
    "from qonnx.transformation.general import (\n",
    "    ConvertSubToAdd,\n",
    "    ConvertDivToMul,\n",
    "    GiveReadableTensorNames,\n",
    "    GiveUniqueNodeNames,\n",
    "    SortGraph,\n",
    "    RemoveUnusedTensors,\n",
    "    GiveUniqueParameterTensors,\n",
    "    RemoveStaticGraphInputs,\n",
    "    ApplyConfig,\n",
    ")\n",
    "\n",
    "model_dir = os.environ['FINN_ROOT'] + \"/notebooks/HLS_unet/onnx_model_01ER3LUnet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284e86c0-541d-435f-9b05-149be04badf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "U_net_model.cpu()\n",
    "export_onnx_path = model_dir + \"/unet_export.onnx\"\n",
    "export_qonnx(U_net_model, torch.randn(1, 2, 6, 6), export_onnx_path)\n",
    "qonnx_cleanup(export_onnx_path, out_file=export_onnx_path)\n",
    "model = ModelWrapper(export_onnx_path)\n",
    "\n",
    "model = model.transform(ConvertQONNXtoFINN())\n",
    "model = model.transform(GiveUniqueParameterTensors())\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(RemoveStaticGraphInputs())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(InferDataTypes())\n",
    "# model = model.transform(InsertTopK(k=1, axis=1))\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(InferDataTypes())\n",
    "model = model.transform(InferDataLayouts())\n",
    "model.save(model_dir + \"/unet.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a8e097-8aa1-4212-8b61-9a88c1b6bce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "showInNetron(model_dir + \"/unet.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddb7ac6-e84c-49d3-81e5-cde9e9bd7546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_resnet50_streamline_linear(model: ModelWrapper):\n",
    "    streamline_transformations = [\n",
    "        MoveLinearPastFork(),\n",
    "        MoveMulPastMaxPool(),\n",
    "        AbsorbSignBiasIntoMultiThreshold(), # Absorb add into MultiThreshold\n",
    "        AbsorbScalarMulAddIntoTopK(),  # before MoveAddPastMul to avoid int->float\n",
    "        ConvertSubToAdd(),\n",
    "        ConvertDivToMul(),\n",
    "        RemoveIdentityOps(),\n",
    "        CollapseRepeatedMul(),\n",
    "        BatchNormToAffine(), # BatchNorm to mul + add (B <1, C, 1, 1>)\n",
    "        ConvertSignToThres(),\n",
    "        MoveAddPastMul(),\n",
    "        MoveScalarAddPastMatMul(),\n",
    "        MoveAddPastConv(),\n",
    "        MoveScalarMulPastMatMul(),\n",
    "        MoveScalarMulPastConv(),\n",
    "        MoveScalarLinearPastInvariants(),\n",
    "        MoveAddPastMul(),\n",
    "        CollapseRepeatedAdd(),\n",
    "        CollapseRepeatedMul(),\n",
    "        AbsorbAddIntoMultiThreshold(),\n",
    "        FactorOutMulSignMagnitude(),\n",
    "        MoveMaxPoolPastMultiThreshold(),\n",
    "        AbsorbMulIntoMultiThreshold(),\n",
    "        Absorb1BitMulIntoMatMul(),\n",
    "        Absorb1BitMulIntoConv(),\n",
    "        RoundAndClipThresholds(),\n",
    "    ]\n",
    "    for trn in streamline_transformations:\n",
    "        model = model.transform(trn)\n",
    "        model = model.transform(GiveUniqueNodeNames())\n",
    "    return model\n",
    "\n",
    "\n",
    "def step_resnet50_streamline_nonlinear(model: ModelWrapper):\n",
    "    streamline_transformations = [\n",
    "        MoveLinearPastEltwiseAdd(),\n",
    "        MoveLinearPastFork(),\n",
    "    ]\n",
    "    for trn in streamline_transformations:\n",
    "        model = model.transform(trn)\n",
    "        model = model.transform(GiveUniqueNodeNames())\n",
    "    return model\n",
    "\n",
    "def step_resnet50_streamline(model: ModelWrapper):\n",
    "\n",
    "    for iter_id in range(4):\n",
    "        model = step_resnet50_streamline_linear(model)\n",
    "        model = step_resnet50_streamline_nonlinear(model)\n",
    "\n",
    "        # big loop tidy up\n",
    "        model = model.transform(RemoveUnusedTensors())\n",
    "        model = model.transform(GiveReadableTensorNames())\n",
    "        model = model.transform(InferDataTypes())\n",
    "        model = model.transform(SortGraph())\n",
    "\n",
    "    model = model.transform(DoubleToSingleFloat())\n",
    "\n",
    "    return model\n",
    "\n",
    "model = ModelWrapper(model_dir + \"/unet.onnx\")\n",
    "model = step_resnet50_streamline(model)\n",
    "model.save(model_dir + \"/unet_streamline.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6f824d-cd90-4802-a871-daf46c8975c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "showInNetron(model_dir + \"/unet_streamline.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee05f607-4ecc-45b6-8ae3-d40d469ffde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_resnet50_convert_to_hls(model: ModelWrapper):\n",
    "    model.set_tensor_datatype(model.graph.input[0].name, DataType[\"UINT4\"])\n",
    "    model = model.transform(InferDataLayouts())\n",
    "\n",
    "    model = model.transform(DoubleToSingleFloat())\n",
    "    model = model.transform(InferDataTypes())\n",
    "    model = model.transform(SortGraph())\n",
    "\n",
    "    to_hls_transformations = [\n",
    "        to_hls.InferAddStreamsLayer,\n",
    "        LowerConvsToMatMul,\n",
    "        to_hls.InferChannelwiseLinearLayer,\n",
    "        MakeMaxPoolNHWC,\n",
    "        to_hls.InferPool_Batch,\n",
    "        AbsorbTransposeIntoMultiThreshold,\n",
    "        RoundAndClipThresholds,\n",
    "        to_hls.InferQuantizedMatrixVectorActivation,\n",
    "        to_hls.InferThresholdingLayer,\n",
    "        AbsorbConsecutiveTransposes,\n",
    "        to_hls.InferConcatLayer,\n",
    "        MakeScaleResizeNHWC,\n",
    "        to_hls.InferUpsample,\n",
    "        to_hls.InferConvInpGen,\n",
    "        to_hls.InferDuplicateStreamsLayer,\n",
    "        #to_hls.InferLabelSelectLayer,\n",
    "        #InferLabelSelectLayer_reshape,\n",
    "        #MoveTransposePastTopK,\n",
    "        AbsorbConsecutiveTransposes,\n",
    "        InferDataLayouts\n",
    "    ]\n",
    "    for trn in to_hls_transformations:\n",
    "        model = model.transform(trn())\n",
    "        model = model.transform(InferDataLayouts())\n",
    "        model = model.transform(GiveUniqueNodeNames())\n",
    "        model = model.transform(InferDataTypes())\n",
    "\n",
    "    model = model.transform(RemoveCNVtoFCFlatten())\n",
    "    model = model.transform(GiveReadableTensorNames())\n",
    "    model = model.transform(RemoveUnusedTensors())\n",
    "    model = model.transform(SortGraph())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d03115-a08d-46d2-84d1-428e29981fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelWrapper(model_dir + \"/unet_streamline.onnx\")\n",
    "model = step_resnet50_convert_to_hls(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c64f2b-4e3d-4729-8ab2-fbbde493595f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_dir + \"/unet_hls.onnx\")\n",
    "# showInNetron(model_dir + \"/unet_hls.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f282fa05-b016-4833-8484-be4e5da06897",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_model = model.transform(CreateDataflowPartition())\n",
    "parent_model.save(model_dir + \"/unet_dataflow_parent.onnx\")\n",
    "#showInNetron(model_dir + \"/unet_dataflow_parent.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4598aa-0d46-4d52-aef9-f1a19ae4639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qonnx.custom_op.registry import getCustomOp\n",
    "sdp_node = parent_model.get_nodes_by_op_type(\"StreamingDataflowPartition\")[0]\n",
    "sdp_node = getCustomOp(sdp_node)\n",
    "dataflow_model_filename = sdp_node.get_nodeattr(\"model\")\n",
    "# save the dataflow partition with a different name for easier access\n",
    "dataflow_model = ModelWrapper(dataflow_model_filename)\n",
    "dataflow_model.save(model_dir + \"/unet_dataflow_model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e93b28-708e-43cf-a217-8a6b3019b3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate MVAU params.h to save weights\n",
    "\n",
    "from qonnx.core.datatype import DataType\n",
    "from qonnx.util.basic import (\n",
    "    calculate_matvec_accumulator_range,\n",
    "    interleave_matrix_outer_dim_from_partitions,\n",
    "    roundup_to_integer_multiple,\n",
    ")\n",
    "from finn.util.data_packing import (\n",
    "    npy_to_rtlsim_input,\n",
    "    numpy_to_hls_code,\n",
    "    pack_innermost_dim_as_hex_string,\n",
    "    rtlsim_output_to_npy,\n",
    ")\n",
    "\n",
    "# set up loaded model and save_path\n",
    "model = ModelWrapper(model_dir + \"/unet_dataflow_model.onnx\")\n",
    "save_path = os.path.join(model_dir, \"MVAU_params_for_dataflow\")\n",
    "save_path_dat = os.path.join(model_dir, \"MVAU_memstream_dat\")\n",
    "save_path_npy = os.path.join(model_dir, \"MVAU_weights_npy\")\n",
    "# Check if the directory exists, if not, create it\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "if not os.path.exists(save_path_npy):\n",
    "    os.makedirs(save_path_npy)\n",
    "if not os.path.exists(save_path_dat):\n",
    "    os.makedirs(save_path_dat)\n",
    "fc_layers = model.get_nodes_by_op_type(\"MatrixVectorActivation\")\n",
    "\n",
    "# TODO: define a list like below to save pe, simd, wmem, ... all the params related to layers\n",
    "# DONE\n",
    "weights_shape = [\n",
    "    (2, 16, 9, False),\n",
    "    (16, 16, 9, False),\n",
    "    \n",
    "    (16, 32, 9, False),\n",
    "    (32, 32, 9, False),\n",
    "    \n",
    "    (32, 32, 18, False),# wmem doubled\n",
    "    (64, 32, 18, False),# wmem doubled\n",
    "    \n",
    "    (64, 32, 1, False),\n",
    "    \n",
    "    (32, 16, 9, False),\n",
    "    (16, 16, 9, False),\n",
    "    \n",
    "    (16, 16, 1, False),\n",
    "    \n",
    "    (16, 8, 9, False),\n",
    "    (8, 8, 9, False),\n",
    "    \n",
    "    (8, 4, 1, False)\n",
    "] # For the hw, the max PE is set to 32, so the output channels larger than 32 should be set to 32 and the according wmem should be doubled.\n",
    "\n",
    "weights_shape_dat = []\n",
    "ws_simd = 8\n",
    "ws_pe = 32\n",
    "\n",
    "for (simd, pe, wmem, is_padding) in weights_shape:\n",
    "    if is_padding:\n",
    "        new_weights_shape = (ws_simd, ws_pe, wmem, is_padding)\n",
    "    else:\n",
    "        new_weights_shape = (ws_simd, ws_pe, int(wmem*simd/ws_simd*pe/ws_pe), is_padding)\n",
    "        \n",
    "    weights_shape_dat.append(new_weights_shape)\n",
    "\n",
    "# weight_file_mode can be selected from {hls_header, decoupled_npy, decoupled_verilog_dat}\n",
    "weight_file_mode = \"hls_header\"\n",
    "\n",
    "for index, (fcl, (simd, pe, wmem, is_padding), (new_simd, new_pe, new_wmem, new_is_padding)) in enumerate(zip(fc_layers, weights_shape, weights_shape_dat)):\n",
    "    fcl_inst = MVAU(fcl)\n",
    "    fcl_inst.set_nodeattr(\"SIMD\", simd)\n",
    "    fcl_inst.set_nodeattr(\"PE\", pe)\n",
    "    fcl_inst.set_nodeattr(\"mem_mode\", \"decoupled\")\n",
    "    fcl_inst.set_nodeattr(\"runtime_writeable_weights\", 0)\n",
    "    fcl_inst.set_nodeattr(\"ram_style\", \"block\")\n",
    "    #fcl_inst.set_nodeattr(\"MW\", 32)\n",
    "    #fcl_inst.set_nodeattr(\"MH\", 64)\n",
    "    #fcl_inst.set_nodeattr(\"mem_mode\", \"internal_decoupled\")\n",
    "    #fcl_inst.generate_params(model, save_path)\n",
    "    \n",
    "    weights = model.get_initializer(fcl_inst.onnx_node.input[1])\n",
    "    weight_tensor = weights.T\n",
    "    if fcl_inst.get_weight_datatype() == DataType[\"BIPOLAR\"]:\n",
    "        # convert bipolar to binary\n",
    "        weight_tensor = (weight_tensor + 1) / 2\n",
    "    if is_padding:\n",
    "        weight_tensor = interleave_matrix_outer_dim_from_partitions(weight_tensor, pe)\n",
    "        weight_tensor = weight_tensor.reshape(1, pe, wmem, simd) #set pe, wmem, simd accordingly\n",
    "        if (pe < new_pe) and (simd <= new_simd):\n",
    "            new_shape = (1, new_pe, wmem, new_simd)\n",
    "            new_weights = np.zeros(new_shape)\n",
    "            new_weights[:, :weight_tensor.shape[1], :, :weight_tensor.shape[3]] = weight_tensor\n",
    "        elif (pe < new_pe) and (simd > new_simd):\n",
    "            new_shape = (1, new_pe, wmem, simd)\n",
    "            new_weights = np.zeros(new_shape)\n",
    "            new_weights[:, :weight_tensor.shape[1], :, :] = weight_tensor\n",
    "            new_weights = new_weights.reshape(1, new_pe, int(wmem*simd/new_simd), new_simd)\n",
    "        elif (pe > new_pe) and (simd < new_simd):\n",
    "            new_shape = (1, pe, wmem, new_simd)\n",
    "            new_weights = np.zeros(new_shape)\n",
    "            new_weights[:, :, :, :weight_tensor.shape[3]] = weight_tensor\n",
    "            new_weights = new_weights.reshape(1, new_pe, int(wmem*pe/new_pe), new_simd)\n",
    "    else:\n",
    "        weight_tensor = interleave_matrix_outer_dim_from_partitions(weight_tensor, pe)\n",
    "        new_weights = weight_tensor.reshape(1, pe, wmem, simd) #set pe, wmem, simd accordingly\n",
    "    \n",
    "    weight_tensor = np.flip(new_weights, axis=-1)\n",
    "    #collected_tensors.append(weight_tensor)\n",
    "    export_wdt = DataType[fcl_inst.get_nodeattr(\"weightDataType\")]\n",
    "    if export_wdt == DataType[\"BIPOLAR\"]:\n",
    "        export_wdt = DataType[\"BINARY\"]\n",
    "    if weight_file_mode == \"hls_header\":\n",
    "        weight_hls_code = numpy_to_hls_code(weight_tensor, export_wdt, \"weights\", True, True)\n",
    "        weight_h_filename = \"{}/params{}.h\".format(save_path, index)\n",
    "        weight_cpp_filename = \"{}/params{}.cpp\".format(save_path, index)\n",
    "        f_weights = open(weight_cpp_filename, \"w\")\n",
    "        f_weights.write(\"#include \\\"params{}.h\\\"\\n\".format(index))\n",
    "        f_weights.write(\"\\n\")\n",
    "        if export_wdt.bitwidth() != 1:\n",
    "            f_weights.write(\n",
    "                \"const FixedPointWeights<{},{},{},{}> weights{} = \\n\".format(\n",
    "                    simd,\n",
    "                    export_wdt.get_hls_datatype_str(),\n",
    "                    pe,\n",
    "                    wmem,\n",
    "                    index\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            f_weights.write(\n",
    "                \"const BinaryWeights<{},{},{}> mvau_weights{} = \\n\".format(\n",
    "                    new_simd,\n",
    "                    new_pe,\n",
    "                    new_wmem,\n",
    "                    index\n",
    "                )\n",
    "            )\n",
    "        #f_weights.write(\"{\")\n",
    "        f_weights.write(weight_hls_code)\n",
    "        #f_weights.write(\"}\")\n",
    "        f_weights.close()\n",
    "\n",
    "        f_weights = open(weight_h_filename, \"w\")\n",
    "        f_weights.write(\"#ifndef PARAMS{}_H\\n\".format(index))\n",
    "        f_weights.write(\"#define PARAMS{}_H\\n\".format(index))\n",
    "        f_weights.write(\"\\n\")\n",
    "        f_weights.write(\"#include \\\"../weights.hpp\\\"\\n\")\n",
    "        f_weights.write(\"\\n\")\n",
    "        f_weights.write(\n",
    "            \"extern const FixedPointWeights<{},{},{},{}> weights{};\\n\".format(\n",
    "                simd,\n",
    "                export_wdt.get_hls_datatype_str(),\n",
    "                pe,\n",
    "                wmem,\n",
    "                index\n",
    "            )\n",
    "        )\n",
    "        f_weights.write(\"\\n\")\n",
    "        f_weights.write(\"#endif\")\n",
    "    elif \"decoupled\" in weight_file_mode:\n",
    "        # create a weight stream for various flavors of decoupled mode:\n",
    "        # transpose weight tensor from (1, PE, WMEM, SIMD) to (1, WMEM, PE, SIMD)\n",
    "        weight_tensor_unflipped = np.transpose(weight_tensor, (0, 2, 1, 3))\n",
    "        # reverse SIMD flip for saving weights in .npy\n",
    "        weight_tensor_simd_flipped = np.flip(weight_tensor_unflipped, axis=-1)\n",
    "        # PE flip for saving weights in .dat\n",
    "        weight_tensor_pe_flipped = np.flip(weight_tensor_unflipped, axis=-2)\n",
    "        # reshape weight tensor (simd_flipped and pe_flipped) to desired shape\n",
    "        # simd_flipped\n",
    "        weight_tensor_simd_flipped = weight_tensor_simd_flipped.reshape(1, -1, new_pe * new_simd)\n",
    "        weight_tensor_simd_flipped = weight_tensor_simd_flipped.copy()\n",
    "        # flipped\n",
    "        weight_tensor_pe_flipped = weight_tensor_pe_flipped.reshape(1, -1, new_pe * new_simd)\n",
    "        weight_tensor_pe_flipped = weight_tensor_pe_flipped.copy()\n",
    "        weight_file_name_npy = \"{}/weights_{}.npy\".format(save_path_npy, index)\n",
    "        weight_file_name_dat = \"{}/memstream_{}.dat\".format(save_path_dat, index)\n",
    "        if weight_file_mode == \"decoupled_npy\":\n",
    "            # save weight stream into npy for cppsim\n",
    "            np.save(weight_file_name_npy, weight_tensor_simd_flipped)\n",
    "        elif weight_file_mode == \"decoupled_verilog_dat\":\n",
    "            # convert weight values into hexstring\n",
    "            weight_width = fcl_inst.get_weightstream_width()\n",
    "            # pad to nearest 4 bits to get hex strings\n",
    "            weight_width_padded = roundup_to_integer_multiple(weight_width, 4)\n",
    "            weight_tensor_pe_flipped = pack_innermost_dim_as_hex_string(\n",
    "                weight_tensor_pe_flipped, export_wdt, weight_width_padded, prefix=\"\"\n",
    "            )\n",
    "            # add zeroes to pad out file to 1024 entries\n",
    "            weight_stream = weight_tensor_pe_flipped.flatten()\n",
    "            weight_stream = weight_stream.copy()\n",
    "            with open(weight_file_name_dat, \"w\") as f:\n",
    "                for val in weight_stream:\n",
    "                    f.write(val + \"\\n\")\n",
    "print(\"Complete.\")\n",
    "model.save(model_dir + \"/unet_dataflow_model_01.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b51e60-9a77-41b1-89ef-21ad70d4d286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate MVAU thresh.h to save activation value\n",
    "\n",
    "# set up loaded model and save_path\n",
    "model = ModelWrapper(model_dir + \"/unet_dataflow_model_01.onnx\")\n",
    "save_path = os.path.join(model_dir, \"MVAU_params\")\n",
    "# Check if the directory exists, if not, create it\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "fc_layers = model.get_nodes_by_op_type(\"MatrixVectorActivation\")\n",
    "\n",
    "\n",
    "thresh_shape = [\n",
    "    (16, 16, 1),\n",
    "    (16, 16, 1),\n",
    "    (32, 32, 1),\n",
    "    (32, 32, 1),\n",
    "    (32, 64, 2),\n",
    "    (32, 64, 2),\n",
    "    (32, 32, 1),\n",
    "    (16, 16, 1),\n",
    "    (16, 16, 1),\n",
    "    (16, 16, 1),\n",
    "    (8, 8, 1),\n",
    "    (8, 8, 1),\n",
    "]\n",
    "for index, (fcl, (pe, mh, tmem)) in enumerate(zip(fc_layers, thresh_shape)):\n",
    "    fcl_inst = MVAU(fcl)\n",
    "    if len(fcl_inst.onnx_node.input) > 2:\n",
    "        thresholds = model.get_initializer(fcl_inst.onnx_node.input[2])\n",
    "        if thresholds is not None:\n",
    "            #fcl_inst.set_nodeattr(\"PE\", pe)\n",
    "            #fcl_inst.set_nodeattr(\"MH\", mh)\n",
    "            # tmem = mh // pe\n",
    "            n_thres_steps = thresholds.shape[1]\n",
    "            threshold_tensor = thresholds\n",
    "            #if threshold_tensor.shape[0] == 1:\n",
    "            #    threshold_tensor = np.tile(threshold_tensor, (mh, 1))\n",
    "            threshold_tensor = interleave_matrix_outer_dim_from_partitions(threshold_tensor, pe)\n",
    "            threshold_tensor = threshold_tensor.reshape(1, pe, tmem, n_thres_steps)\n",
    "            tdt = DataType[fcl_inst.get_nodeattr(\"accDataType\")]\n",
    "            print(tdt)\n",
    "            thresholds_hls_code = numpy_to_hls_code(threshold_tensor, tdt, \"thresholds\", False, True)\n",
    "            tdt_hls = tdt.get_hls_datatype_str()\n",
    "            tdt_hls_usrset = \"ap_int<12>\"\n",
    "            # use binary to export bipolar activations\n",
    "            export_odt = fcl_inst.get_output_datatype()\n",
    "            odt_hls = export_odt.get_hls_datatype_str()\n",
    "            \n",
    "            # write thresholds into thresh.h\n",
    "            thresh_h_filename = \"{}/MVAU_thresh{}.h\".format(save_path, index)\n",
    "            thresh_cpp_filename = \"{}/MVAU_thresh{}.cpp\".format(save_path, index)\n",
    "# cpp file  \n",
    "            f_thresh = open(thresh_cpp_filename, \"w\")\n",
    "            f_thresh.write(\"#include \\\"MVAU_thresh{}.h\\\"\\n\".format(index))\n",
    "            f_thresh.write(\"\\n\")\n",
    "            f_thresh.write(\n",
    "                \"ThresholdsActivation<{},{},{},{},{},{},{}> mvau_threshs{} = \\n\".format(\n",
    "                    tmem,\n",
    "                    pe,\n",
    "                    threshold_tensor.shape[-1],\n",
    "                    tdt_hls_usrset,\n",
    "                    odt_hls,\n",
    "                    fcl_inst.get_nodeattr(\"ActVal\"),\n",
    "                    \"comp::less_equal<%s, %s>\" % (tdt_hls_usrset, tdt_hls_usrset),\n",
    "                    index,\n",
    "                )\n",
    "            )\n",
    "            f_thresh.write(thresholds_hls_code)\n",
    "            f_thresh.close()\n",
    "# header file\n",
    "            f_thresh = open(thresh_h_filename, \"w\")\n",
    "            f_thresh.write(\"#ifndef MVAU_THRESH{}_H\\n\".format(index))\n",
    "            f_thresh.write(\"#define MVAU_THRESH{}_H\\n\".format(index))\n",
    "            f_thresh.write(\"\\n\")\n",
    "            f_thresh.write(\"#include \\\"../activations.hpp\\\"\\n\")\n",
    "            f_thresh.write(\"\\n\")\n",
    "            f_thresh.write(\n",
    "                \"extern ThresholdsActivation<{},{},{},{},{},{},{}> mvau_threshs{};\\n\".format(\n",
    "                    tmem,\n",
    "                    pe,\n",
    "                    threshold_tensor.shape[-1],\n",
    "                    tdt_hls_usrset,\n",
    "                    odt_hls,\n",
    "                    fcl_inst.get_nodeattr(\"ActVal\"),\n",
    "                    \"comp::less_equal<%s, %s>\" % (tdt_hls_usrset, tdt_hls_usrset),\n",
    "                    index,\n",
    "                )\n",
    "            )\n",
    "            f_thresh.write(\"\\n\")\n",
    "            f_thresh.write(\"#endif\\n\")\n",
    "            f_thresh.close()\n",
    "\n",
    "model.save(model_dir + \"/unet_dataflow_model_02.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267b4dbb-0c89-43d0-a346-080ce8f901c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Thresholding_Batch thresh.h to save activation value\n",
    "\n",
    "model = ModelWrapper(model_dir + \"/unet_dataflow_model_02.onnx\")\n",
    "save_path = os.path.join(model_dir, \"Thresholding_params\")\n",
    "# Check if the directory exists, if not, create it\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "Thresholding_layers = model.get_nodes_by_op_type(\"Thresholding_Batch\")\n",
    "\n",
    "\n",
    "thresh_shape = [\n",
    "    (2, 2, 1),\n",
    "    (16, 16, 1),\n",
    "    (32, 32, 1),\n",
    "    (32, 32, 1),\n",
    "    (32, 32, 1),\n",
    "    (16, 16, 1),\n",
    "    (16, 16, 1),\n",
    "]\n",
    "for index, (fcl, (pe, mh, tmem)) in enumerate(zip(Thresholding_layers, thresh_shape)):\n",
    "    fcl_inst = Thresholding_Batch(fcl)\n",
    "    fcl_inst.set_nodeattr(\"PE\", pe)\n",
    "    fcl_inst.set_nodeattr(\"NumChannels\", mh)\n",
    "    fcl_inst.set_nodeattr(\"weightDataType\", \"INT32\")\n",
    "    thresholds = model.get_initializer(fcl_inst.onnx_node.input[1])\n",
    "    n_thres_steps = thresholds.shape[1]\n",
    "    assert n_thres_steps == fcl_inst.get_nodeattr(\"numSteps\"), \"Mismatch in threshold steps\"\n",
    "    if not fcl_inst.get_input_datatype().signed():\n",
    "        # ensure all thresholds are nonnegative\n",
    "        assert (thresholds >= 0).all()\n",
    "    # ensure all thresholds are integer\n",
    "    assert np.equal(np.mod(thresholds, 1), 0).all(), \"Need int threshold tensor\"\n",
    "    #fcl_inst.set_nodeattr(\"PE\", pe)\n",
    "    #fcl_inst.set_nodeattr(\"MH\", mh)\n",
    "    # tmem = mh // pe\n",
    "    threshold_tensor = thresholds\n",
    "    if threshold_tensor.shape[0] == 1:\n",
    "        threshold_tensor = np.tile(threshold_tensor, (mh, 1))\n",
    "    threshold_tensor = interleave_matrix_outer_dim_from_partitions(threshold_tensor, pe)\n",
    "    threshold_tensor = threshold_tensor.reshape(1, pe, tmem, n_thres_steps)\n",
    "    tdt = fcl_inst.get_weight_datatype()\n",
    "    print(tdt)\n",
    "    thresholds_hls_code = numpy_to_hls_code(threshold_tensor, tdt, \"thresholds\", False, True)\n",
    "    tdt_hls = tdt.get_hls_datatype_str()\n",
    "    if tdt_hls == \"ap_int<32>\":\n",
    "        tdt_hls = \"ap_int<8>\"\n",
    "    tdt_hls_usrset = \"ap_int<8>\"\n",
    "    # use binary to export bipolar activations\n",
    "    export_odt = fcl_inst.get_output_datatype()\n",
    "    odt_hls = export_odt.get_hls_datatype_str()\n",
    "    \n",
    "    thresh_h_filename = \"{}/Thresholding_thresh{}.h\".format(save_path, index)\n",
    "    thresh_cpp_filename = \"{}/Thresholding_thresh{}.cpp\".format(save_path, index)\n",
    "# Write to cpp file which contain the data\n",
    "    f_thresh = open(thresh_cpp_filename, \"w\")\n",
    "    f_thresh.write(\"#include \\\"Thresholding_thresh{}.h\\\"\\n\".format(index))\n",
    "    f_thresh.write(\"\\n\")\n",
    "    f_thresh.write(\n",
    "        \"ThresholdsActivation<{},{},{},{},{},{},{}> thresB_threshs{} = \\n\".format(\n",
    "            tmem,\n",
    "            pe,\n",
    "            threshold_tensor.shape[-1],\n",
    "            tdt_hls,\n",
    "            odt_hls,\n",
    "            fcl_inst.get_nodeattr(\"ActVal\"),\n",
    "            \"comp::less_equal<%s, %s>\" % (tdt_hls, tdt_hls),\n",
    "            index,\n",
    "        )\n",
    "    )\n",
    "    f_thresh.write(thresholds_hls_code)\n",
    "    f_thresh.close()\n",
    "# Write to header file\n",
    "    f_thresh = open(thresh_h_filename, \"w\")\n",
    "    tdt_hls = tdt.get_hls_datatype_str()\n",
    "    if tdt_hls == \"ap_int<32>\":\n",
    "        tdt_hls = \"ap_int<8>\"\n",
    "    tdt_hls_usrset = \"ap_int<8>\"\n",
    "    # use binary to export bipolar activations\n",
    "    export_odt = fcl_inst.get_output_datatype()\n",
    "    odt_hls = export_odt.get_hls_datatype_str()\n",
    "    f_thresh.write(\"#ifndef THRESHOLDING_THRESH{}_H\\n\".format(index))\n",
    "    f_thresh.write(\"#define THRESHOLDING_THRESH{}_H\\n\".format(index))\n",
    "    f_thresh.write(\"\\n\")\n",
    "    f_thresh.write(\"#include \\\"../activations.hpp\\\"\\n\")\n",
    "    f_thresh.write(\"\\n\")\n",
    "    f_thresh.write(\n",
    "        \"extern ThresholdsActivation<{},{},{},{},{},{},{}> thresB_threshs{};\\n\".format(\n",
    "            tmem,\n",
    "            pe,\n",
    "            threshold_tensor.shape[-1],\n",
    "            tdt_hls,\n",
    "            odt_hls,\n",
    "            fcl_inst.get_nodeattr(\"ActVal\"),\n",
    "            \"comp::less_equal<%s, %s>\" % (tdt_hls, tdt_hls),\n",
    "            index,\n",
    "        )\n",
    "    )\n",
    "    f_thresh.write(\"\\n\")\n",
    "    f_thresh.write(\"#endif\\n\")\n",
    "    #f_thresh.write(thresholds_hls_code)\n",
    "    f_thresh.close()\n",
    "\n",
    "model.save(model_dir + \"/unet_dataflow_model_03.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdaed19-4908-45fe-be90-a2db82301e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.util.basic import pynq_part_map\n",
    "#print(pynq_part_map.keys())\n",
    "\n",
    "pynq_board = \"RFSoC4x2\"\n",
    "fpga_part = pynq_part_map[pynq_board]\n",
    "target_clk_ns = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0aa1b71-2dc9-46a2-accc-6c4186a49a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.fpgadataflow.make_zynq_proj import ZynqBuild\n",
    "model = ModelWrapper(model_dir + \"/unet_dataflow_model_03.onnx\")\n",
    "model = model.transform(InsertAndSetFIFODepths(fpga_part))\n",
    "model.save(model_dir + \"/unet_dataflow_model_03_withfifo.onnx\")\n",
    "model = ModelWrapper(model_dir + \"/unet_dataflow_model_03_withfifo.onnx\")\n",
    "model = model.transform(ZynqBuild(platform = pynq_board, period_ns = target_clk_ns))\n",
    "model.save(model_dir + \"/unet_dataflow_model_03_ZynqBuild.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64db62a3-f40e-47d5-b266-b2041faff01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.fpgadataflow.make_pynq_driver import MakePYNQDriver\n",
    "model = ModelWrapper(model_dir + \"/unet_dataflow_model_03_ZynqBuild.onnx\")\n",
    "model = model.transform(MakePYNQDriver(\"zynq-iodma\"))\n",
    "model.save(model_dir + \"/unet_synth.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76d8f77-021c-4ee1-834c-6e0c82e05231",
   "metadata": {},
   "outputs": [],
   "source": [
    "showInNetron(model_dir + \"/unet_synth.onnx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
